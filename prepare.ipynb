{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装软件包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: numpy in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: tqdm in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: tensorboardx in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: pyyaml in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: torch in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: pillow in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (11.1.0)\n",
      "Collecting flax\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/50/a2/daca2bc563e1fd53c33fbff1e33e84004639f7ad9e1a9a54370480a7780d/flax-0.10.2-py3-none-any.whl (424 kB)\n",
      "Requirement already satisfied: jax[cuda12] in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (0.4.38)\n",
      "Requirement already satisfied: packaging in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from tensorboardx) (24.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from tensorboardx) (5.29.3)\n",
      "Requirement already satisfied: filelock in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: jaxlib<=0.4.38,>=0.4.38 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from jax[cuda12]) (0.4.38)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from jax[cuda12]) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from jax[cuda12]) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from jax[cuda12]) (1.15.1)\n",
      "Requirement already satisfied: jax-cuda12-plugin<=0.4.38,>=0.4.38 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.38,>=0.4.38; extra == \"cuda12\"->jax[cuda12]) (0.4.38)\n",
      "Collecting msgpack (from flax)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/f1/54/65af8de681fa8255402c80eda2a501ba467921d5a7a028c9c22a2c2eedb5/msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "Collecting optax (from flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/5c/24/28d0bb21600a78e46754947333ec9a297044af884d360092eb8561575fe9/optax-0.2.4-py3-none-any.whl (319 kB)\n",
      "Collecting orbax-checkpoint (from flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/87/32/3779fa524a2272f408ab51d869fde9ff1c0ca731eedd01e40436bcf7ba2c/orbax_checkpoint-0.11.0-py3-none-any.whl (360 kB)\n",
      "Collecting tensorstore (from flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/be/79/94decc1a93a4b53be8ef1fada387bbc1cf89475fe8da5a6dae6c74bf1f01/tensorstore-0.1.71-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.8 MB)\n",
      "Collecting rich>=11.1 (from flax)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/19/71/39c7c0d87f8d4e6c020a393182060eaefeeae6c01dab6a84ec346f2567df/rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Requirement already satisfied: jax-cuda12-pjrt==0.4.38 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from jax-cuda12-plugin<=0.4.38,>=0.4.38->jax-cuda12-plugin[with_cuda]<=0.4.38,>=0.4.38; extra == \"cuda12\"->jax[cuda12]) (0.4.38)\n",
      "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.6.85 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.38,>=0.4.38; extra == \"cuda12\"->jax[cuda12]) (12.6.85)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from rich>=11.1->flax) (2.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting absl-py>=0.7.1 (from optax->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting chex>=0.1.87 (from optax->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/58/a0/e4bebe76bdd0a68077030f1b5e48b545597473ae1b773c84150311152efc/chex-0.1.88-py3-none-any.whl (99 kB)\n",
      "Collecting etils[epy] (from optax->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/4f/4a/ff8aa2c57300613b69905308c5afe92c5b01112d766c25a305fd6796170a/etils-1.11.0-py3-none-any.whl (165 kB)\n",
      "Requirement already satisfied: nest_asyncio in /home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Collecting humanize (from orbax-checkpoint->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/92/75/4bc3e242ad13f2e6c12e0b0401ab2c5e5c6f0d7da37ec69bc808e24e0ccb/humanize-4.11.0-py3-none-any.whl (128 kB)\n",
      "Collecting simplejson>=3.16.0 (from orbax-checkpoint->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/37/e3/663a09542ee021d4131162f7a164cb2e7f04ef48433a67591738afbf12ea/simplejson-3.19.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Collecting toolz>=0.9.0 (from chex>=0.1.87->optax->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/03/98/eb27cc78ad3af8e302c9d8ff4977f5026676e130d28dd7578132a457170c/toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting importlib_resources (from etils[epath,epy]->orbax-checkpoint->flax)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a4/ed/1f1afb2e9e7f38a545d628f864d562a5ae64fe6f7a10e28ffb9b185b4e89/importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting zipp (from etils[epath,epy]->orbax-checkpoint->flax)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/b7/1a/7e4798e9339adc931158c9d69ecc34f5e6791489d469f5e50ec15e35f458/zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: zipp, toolz, simplejson, msgpack, mdurl, importlib_resources, humanize, etils, absl-py, tensorstore, markdown-it-py, rich, orbax-checkpoint, chex, optax, flax\n",
      "Successfully installed absl-py-2.1.0 chex-0.1.88 etils-1.11.0 flax-0.10.2 humanize-4.11.0 importlib_resources-6.5.2 markdown-it-py-3.0.0 mdurl-0.1.2 msgpack-1.1.0 optax-0.2.4 orbax-checkpoint-0.11.0 rich-13.9.4 simplejson-3.19.3 tensorstore-0.1.71 toolz-1.0.0 zipp-3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U numpy tqdm tensorboardx pyyaml torch pillow \"jax[cuda12]\" flax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取NEU Seg数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'data'...\n",
      "remote: Enumerating objects: 8947, done.\u001b[K\n",
      "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 8947 (delta 0), reused 0 (delta 0), pack-reused 8945 (from 1)\u001b[K\n",
      "接收对象中: 100% (8947/8947), 35.08 MiB | 5.38 MiB/s, 完成.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/DHW-Master/NEU_Seg.git data && rm -rf data/.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据集进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 文件夹路径\n",
    "train_image_dir = 'data/images/training'\n",
    "train_mask_dir = 'data/annotations/training'\n",
    "test_image_dir = 'data/images/test'\n",
    "test_mask_dir = 'data/annotations/test'\n",
    "\n",
    "# 获取文件列表\n",
    "train_images = sorted(os.listdir(train_image_dir))\n",
    "train_masks = sorted(os.listdir(train_mask_dir))\n",
    "test_images = sorted(os.listdir(test_image_dir))\n",
    "test_masks = sorted(os.listdir(test_mask_dir))\n",
    "\n",
    "# 确保文件对应\n",
    "assert len(train_images) == len(train_masks), \"训练图片和掩码数量不一致\"\n",
    "assert len(test_images) == len(test_masks), \"测试图片和掩码数量不一致\"\n",
    "\n",
    "# 完整路径\n",
    "train_image_paths = [os.path.join(train_image_dir, img) for img in train_images]\n",
    "train_mask_paths = [os.path.join(train_mask_dir, msk) for msk in train_masks]\n",
    "test_image_paths = [os.path.join(test_image_dir, img) for img in test_images]\n",
    "test_mask_paths = [os.path.join(test_mask_dir, msk) for msk in test_masks]\n",
    "\n",
    "def load_images(paths):\n",
    "    data = []\n",
    "    for path in paths:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        # img_array = np.transpose(img_array, (2, 0, 1))  # 将通道轴移到最前面\n",
    "        data.append(img_array)\n",
    "    return np.array(data)\n",
    "\n",
    "def load_masks(paths):\n",
    "    data = []\n",
    "    for path in paths:\n",
    "        img = Image.open(path).convert('L')\n",
    "        data.append(np.array(img))\n",
    "    return np.array(data)\n",
    "\n",
    "# 加载并保存训练集\n",
    "train_images = load_images(train_image_paths)\n",
    "train_masks = load_masks(train_mask_paths)\n",
    "np.save('data/train_images.npy', train_images)\n",
    "np.save('data/train_masks.npy', train_masks)\n",
    "\n",
    "# 加载并保存测试集\n",
    "test_images = load_images(test_image_paths)\n",
    "test_masks = load_masks(test_mask_paths)\n",
    "np.save('data/test_images.npy', test_images)\n",
    "np.save('data/test_masks.npy', test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确认jax状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX running on GPU\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# Show on which platform JAX is running.\n",
    "print(\"JAX running on\", jax.devices()[0].platform.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/500]\n",
      "  0%|                                                   | 0/227 [00:00<?, ?it/s]2025-01-17 22:33:34.634812: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 25.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:35.270586: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 50.41GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:36.102375: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 19.44GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:36.185844: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:36.538083: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 75.98GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:36.633161: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 25.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:36.703396: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 25.24GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:36.883412: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 25.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:37.101832: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 25.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-17 22:33:37.362485: W external/xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 75.60GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "100%|█████████████████████████████████████████| 227/227 [01:00<00:00,  3.78it/s]\n",
      "loss: 0.2037183940410614\n",
      "  0%|                                                    | 0/53 [00:07<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pdch/workspace/jax sample repository/main.py\", line 170, in <module>\n",
      "    logits = jnp.array(logits, device=jax.devices(\"cpu\")[0])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py\", line 5653, in array\n",
      "    out_array: Array = lax_internal._convert_element_type(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages/jax/_src/lax/lax.py\", line 608, in _convert_element_type\n",
      "    return convert_element_type_p.bind(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages/jax/_src/core.py\", line 463, in bind\n",
      "    return self.bind_with_trace(prev_trace, args, params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages/jax/_src/lax/lax.py\", line 3081, in _convert_element_type_bind_with_trace\n",
      "    operand = core.Primitive.bind_with_trace(convert_element_type_p, trace, args, params)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages/jax/_src/core.py\", line 468, in bind_with_trace\n",
      "    return trace.process_primitive(self, args, params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages/jax/_src/core.py\", line 941, in process_primitive\n",
      "    return primitive.impl(*args, **params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pdch/miniforge3/envs/jax/lib/python3.12/site-packages/jax/_src/dispatch.py\", line 90, in apply_primitive\n",
      "    outs = fun(*args)\n",
      "           ^^^^^^^^^^\n",
      "ValueError: Received incompatible devices for jitted computation. Got argument args[0] of convert_element_type with shape float32[16,200,200,4] and device ids [0] on platform GPU and sharding_constraint inside jit with device ids [0] on platform CPU at /home/pdch/workspace/jax sample repository/main.py:170:25 (<module>)\n",
      "--------------------\n",
      "For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清理log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf work_dir/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
